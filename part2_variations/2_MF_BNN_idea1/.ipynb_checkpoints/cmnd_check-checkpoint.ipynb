{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.66772841]\n",
      " [-2.46244005]]\n",
      "[[-2.11939794 -2.51323535]\n",
      " [-4.01385615 -2.80462661]]\n",
      "[[-2.11939794]\n",
      " [-4.01385615]]\n",
      "[[-2.51323535]\n",
      " [-2.80462661]]\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 10\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "weights_in_1 = np.random.randn(1, n_hidden).astype(floatX)\n",
    "weights_1_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "weights_2_out = np.random.randn(n_hidden,2).astype(floatX)\n",
    "\n",
    "bias_in_1 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_1_2 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_2_out = np.random.randn(2).astype(floatX)\n",
    "\n",
    "\n",
    "def DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out):\n",
    "    act_1 = pm.math.tanh(pm.math.dot(x_tensor,weights_in_1)+bias_in_1)\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,weights_1_2)+bias_1_2)\n",
    "    act_out = pm.math.dot(act_2,weights_2_out)+bias_2_out\n",
    "    return act_out\n",
    "\n",
    "\n",
    "x_tensor = np.random.randn(2,1)\n",
    "print (x_tensor) \n",
    "t = DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out).eval()\n",
    "print (t)\n",
    "print (t[:,0][:,None])\n",
    "print (t[:,1][:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.81085375 0.3553279 ]\n"
     ]
    }
   ],
   "source": [
    "print (t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_l = 50,12 # number of low fidelity data observations\n",
    "N_h = 13,5 # number of high fidelity data observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.55590085]\n",
      " [-1.86220091]]\n",
      "[[-0.09105339  2.98473423  0.98569698  0.83642283]\n",
      " [ 1.2159812  -4.88129871 -0.50876703 -2.28190528]]\n",
      "[[[-0.09105339  2.98473423]]\n",
      "\n",
      " [[ 1.2159812  -4.88129871]]] (2, 2)\n",
      "[[[ 0.98569698  0.83642283]]\n",
      "\n",
      " [[-0.50876703 -2.28190528]]]\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 10\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "weights_in_1 = np.random.randn(1, n_hidden).astype(floatX)\n",
    "weights_1_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "weights_2_out = np.random.randn(n_hidden,4).astype(floatX)\n",
    "\n",
    "bias_in_1 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_1_2 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_2_out = np.random.randn(4).astype(floatX)\n",
    "\n",
    "\n",
    "def DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out):\n",
    "    act_1 = pm.math.tanh(pm.math.dot(x_tensor,weights_in_1)+bias_in_1)\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,weights_1_2)+bias_1_2)\n",
    "    act_out = pm.math.dot(act_2,weights_2_out)+bias_2_out\n",
    "    return act_out\n",
    "\n",
    "\n",
    "x_tensor = np.random.randn(2,1)\n",
    "print (x_tensor) \n",
    "t = DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out).eval()\n",
    "print (t)\n",
    "print (t[:,:2][:,None], t[:,:2].shape)\n",
    "print (t[:,-2:][:,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.10989906]\n",
      " [0.70776029]]\n",
      "[[2.11166689 3.09742941]\n",
      " [0.01024066 0.81388516]]\n",
      "[2.11166689 0.01024066]\n",
      "[3.09742941 0.81388516]\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 10\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "weights_in_1 = np.random.randn(1, n_hidden).astype(floatX)\n",
    "weights_1_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "weights_2_out = np.random.randn(n_hidden,2).astype(floatX)\n",
    "\n",
    "bias_in_1 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_1_2 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_2_out = np.random.randn(2).astype(floatX)\n",
    "\n",
    "\n",
    "def DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out):\n",
    "    act_1 = pm.math.tanh(pm.math.dot(x_tensor,weights_in_1)+bias_in_1)\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,weights_1_2)+bias_1_2)\n",
    "    act_out = pm.math.dot(act_2,weights_2_out)+bias_2_out\n",
    "    return act_out\n",
    "\n",
    "\n",
    "x_tensor = np.random.randn(2,1)\n",
    "print (x_tensor) \n",
    "t = DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out).eval()\n",
    "print (t)\n",
    "print (t[:,0])\n",
    "print (t[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stat = 200 # number of prediction samples to calculate statistics\n",
    "\n",
    "def posterior_sampler(j):\n",
    "    Ws_post = []\n",
    "    bs_post = []\n",
    "    for i in range(len(d)-1):\n",
    "        W_post = trace['W_'+str(i)][ j ]\n",
    "        b_post = trace['b_'+str(i)][ j ]\n",
    "        Ws_post.append(W_post)\n",
    "        bs_post.append(b_post)\n",
    "    return Ws_post, bs_post\n",
    "\n",
    "preds_low = []\n",
    "preds_high = []\n",
    "for j in range(n_stat):\n",
    "    Ws_post, bs_post = posterior_sampler(j)\n",
    "    preds = DNN( X_p, Ws_post, bs_post ).eval() \n",
    "    preds_low.append( preds[:,0][:,None] )\n",
    "    preds_high.append( preds[:,1][:,None] )\n",
    "\n",
    "preds_low = np.asarray(preds_low)\n",
    "outputs_low = preds_low.reshape(preds_low.shape[0],preds_low.shape[1])\n",
    "print(np.shape(outputs_low))\n",
    "\n",
    "preds_high = np.asarray(preds_high)\n",
    "outputs_high = preds_high.reshape(preds_high.shape[0],preds_high.shape[1])\n",
    "print(np.shape(outputs_high))\n",
    "\n",
    "# #########\n",
    "n_plot = 10 # number of prediction samples to plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(X_p, f_l(X_p), 'r--', label='True LF response')\n",
    "ax.scatter(X_l, Y_l, color='red', marker='o', s=100, label='LF data')\n",
    "ax.plot(X_p,outputs_low[0].T, lw=1, label='LF posterior draws', color='red')\n",
    "ax.plot(X_p,outputs_low[1:n_plot].T, lw=1, color='red')\n",
    "\n",
    "ax.plot(X_p, f_h(X_p), 'b--', label='True HF response')\n",
    "ax.scatter(X_h, Y_h, color='blue', marker='x', s=100, label='HF data')\n",
    "ax.plot(X_p,outputs_high[0].T, lw=1, label='HF posterior draws',color='blue')\n",
    "ax.plot(X_p,outputs_high[1:n_plot].T, lw=1, color='blue')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n",
    "# #########\n",
    "\n",
    "mean_outputs_low = np.mean(outputs_low, axis=0)\n",
    "stddev_outputs_low = np.std(outputs_low, axis=0)\n",
    "mean_outputs_high = np.mean(outputs_high, axis=0)\n",
    "stddev_outputs_high = np.std(outputs_high, axis=0)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(X_p, f_l(X_p), 'r--', label='True LF response')\n",
    "ax.scatter(X_l, Y_l, color='red', marker='o', s=100, label='LF data')\n",
    "ax.plot(X_p, mean_outputs_low ,lw=2, label='LF mean', color='red')\n",
    "ax.fill_between(X_p.ravel(), (mean_outputs_low-1.96*stddev_outputs_low).ravel(), (mean_outputs_low+1.96*stddev_outputs_low).ravel(), alpha = 0.3, color = 'red',label='LF epistemic uncertanity')\n",
    "\n",
    "ax.plot(X_p, f_h(X_p), 'b--', label='True HF response')\n",
    "ax.scatter(X_h, Y_h, color='blue', marker='o', s=100, label='HF data')\n",
    "ax.plot(X_p, mean_outputs_high ,lw=2, label='HF mean', color='blue')\n",
    "ax.fill_between(X_p.ravel(), (mean_outputs_high-1.96*stddev_outputs_high).ravel(), (mean_outputs_high+1.96*stddev_outputs_high).ravel(), alpha = 0.3, color = 'blue',label='HF epistemic uncertanity')\n",
    "\n",
    "plt.title('n_samples='+str(n_stat))\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
