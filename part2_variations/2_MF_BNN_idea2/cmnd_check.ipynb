{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "np.random.seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0946745 ]\n",
      " [ 2.33783541]]\n",
      "[[-0.52626916  4.33038502  0.06470451  0.82676254]\n",
      " [ 0.19304078 -1.62994765  0.63786149  3.68035277]]\n",
      "[[-0.52626916  4.33038502]\n",
      " [ 0.19304078 -1.62994765]]\n",
      "[[0.06470451 0.82676254]\n",
      " [0.63786149 3.68035277]]\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 10\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "weights_in_1 = np.random.randn(1, n_hidden).astype(floatX)\n",
    "weights_1_2 = np.random.randn(n_hidden, n_hidden).astype(floatX)\n",
    "weights_2_out = np.random.randn(n_hidden,4).astype(floatX)\n",
    "\n",
    "bias_in_1 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_1_2 = np.random.randn(n_hidden).astype(floatX)\n",
    "bias_2_out = np.random.randn(4).astype(floatX)\n",
    "\n",
    "\n",
    "def DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out):\n",
    "    act_1 = pm.math.tanh(pm.math.dot(x_tensor,weights_in_1)+bias_in_1)\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,weights_1_2)+bias_1_2)\n",
    "    act_out = pm.math.dot(act_2,weights_2_out)+bias_2_out\n",
    "    return act_out\n",
    "\n",
    "\n",
    "x_tensor = np.random.randn(2,1)\n",
    "print (x_tensor) \n",
    "t = DNN(x_tensor, weights_in_1, bias_in_1, weights_1_2, bias_1_2, weights_2_out, bias_2_out).eval()\n",
    "print (t)\n",
    "print (t[:,:2])\n",
    "print (t[:,-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.81085375 0.3553279 ]\n"
     ]
    }
   ],
   "source": [
    "print (t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_l = 50,12 # number of low fidelity data observations\n",
    "N_h = 13,5 # number of high fidelity data observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "H = 10\n",
    "print (int(H/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.17981215]\n",
      " [-0.90856332]]\n",
      "[[-0.99856066 -0.94749606]\n",
      " [ 0.23205524 -0.21077209]]\n"
     ]
    }
   ],
   "source": [
    "# Building pymc model\n",
    "\n",
    "H = 4\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "W_0 = np.random.randn(1, H).astype(floatX)\n",
    "W_1 = np.random.randn(H, H).astype(floatX)\n",
    "b_0 = np.random.randn(H).astype(floatX)\n",
    "b_1 = np.random.randn(H).astype(floatX)\n",
    "\n",
    "\n",
    "W_00 = np.random.randn(int(H/2), H).astype(floatX)\n",
    "W_10 = np.random.randn(H, 1).astype(floatX)\n",
    "b_00 = np.random.randn(H).astype(floatX)\n",
    "b_10 = np.random.randn(1).astype(floatX)\n",
    "\n",
    "W_01 = np.random.randn(int(H/2), H).astype(floatX)\n",
    "W_11 = np.random.randn(H, 1).astype(floatX)\n",
    "b_01 = np.random.randn(H).astype(floatX)\n",
    "b_11 = np.random.randn(1).astype(floatX)\n",
    "\n",
    "def DNN(x_tensor, W_0, b_0, W_1, b_1,W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11 ):\n",
    "    act_1 = pm.math.tanh(pm.math.dot(x_tensor,W_0)+b_0)\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,W_1)+b_1)\n",
    "#     act_2_0 = act_2[:,:int(H/2)]\n",
    "#     act_2_1 = act_2[:,int(-H/2):]\n",
    "    \n",
    "#     act_10 = pm.math.tanh(pm.math.dot(act_2_0,W_00)+b_00)\n",
    "#     act_20 = pm.math.dot(act_10,W_10)+b_10\n",
    "    \n",
    "#     act_11 = pm.math.tanh(pm.math.dot(act_2_1,W_01)+b_01)\n",
    "#     act_21 = pm.math.dot(act_11,W_11)+b_11\n",
    "    \n",
    "    return (act_2[:,-2:])\n",
    "\n",
    "\n",
    "\n",
    "x_tensor = np.random.randn(2,1)\n",
    "print (x_tensor) \n",
    "t = DNN(x_tensor, W_0, b_0, W_1, b_1,W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11).eval()\n",
    "print (t)\n",
    "# print (t[:,0][:,None])\n",
    "# print (t[:,1][:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ref 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-f1fda7b8b18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Initialize random weights between each layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minit_W_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0minit_W_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0minit_b_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloatX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_l' is not defined"
     ]
    }
   ],
   "source": [
    "# Building pymc model\n",
    "\n",
    "H = 10\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "init_W_0 = np.random.randn(X_l.shape[1], H).astype(floatX)\n",
    "init_W_1 = np.random.randn(H, H).astype(floatX)\n",
    "init_b_0 = np.random.randn(H).astype(floatX)\n",
    "init_b_1 = np.random.randn(H).astype(floatX)\n",
    "\n",
    "\n",
    "init_W_00 = np.random.randn(int(H/2), H).astype(floatX)\n",
    "init_W_10 = np.random.randn(H, 1).astype(floatX)\n",
    "init_b_00 = np.random.randn(H).astype(floatX)\n",
    "init_b_10 = np.random.randn(1).astype(floatX)\n",
    "\n",
    "init_W_01 = np.random.randn(int(H/2), H).astype(floatX)\n",
    "init_W_11 = np.random.randn(H, 1).astype(floatX)\n",
    "init_b_01 = np.random.randn(H).astype(floatX)\n",
    "init_b_11 = np.random.randn(1).astype(floatX)\n",
    "\n",
    "def DNN(x_tensor, W_0, b_0, W_1, b_1, W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11 ):\n",
    "    act_1 = pm.math.tanh(pm.math.dot(x_tensor,W_0)+b_0)\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,W_1)+b_1)\n",
    "    act_2_0 = act_2[:,:int(H/2)]\n",
    "    act_2_1 = act_2[:,int(-H/2):]\n",
    "    \n",
    "    act_10 = pm.math.tanh(pm.math.dot(act_2_0,W_00)+b_00)\n",
    "    act_20 = pm.math.dot(act_10,W_10)+b_10\n",
    "    \n",
    "    act_11 = pm.math.tanh(pm.math.dot(act_2_1,W_01)+b_01)\n",
    "    act_21 = pm.math.dot(act_11,W_11)+b_11\n",
    "    \n",
    "    return act_20,act_21\n",
    "\n",
    "with pm.Model() as neural_network:\n",
    "    # Trick: Turning inputs and outputs into shared variables \n",
    "    # It's still the same thing, but we can later change the values of the shared variable\n",
    "    # (to switch in the test-data later) and pymc3 will just use the new data.\n",
    "    # Kind-of like a pointer we can redirect.\n",
    "    # For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "    y_tensor_low = theano.shared(Y_l)\n",
    "    x_tensor_low = theano.shared(X_l)\n",
    "    \n",
    "    y_tensor_high = theano.shared(Y_h)\n",
    "    x_tensor_high = theano.shared(X_h)\n",
    "\n",
    "    #PRIOR\n",
    "    # BNN 1 \n",
    "    W_0 = pm.Normal('W_0', 0, sigma=1,\n",
    "                             shape=(X_l.shape[1], H),\n",
    "                             testval=init_W_0)\n",
    "    \n",
    "    b_0 = pm.Normal('b_0', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_0)\n",
    "\n",
    "    W_1 = pm.Normal('W_1', 0, sigma=1,\n",
    "                            shape=(H, H),\n",
    "                            testval=init_W_1)\n",
    "  \n",
    "    b_1 = pm.Normal('b_1', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_1)\n",
    "    \n",
    "    # BNN 2\n",
    "    W_00 = pm.Normal('W_00', 0, sigma=1,\n",
    "                             shape=(int(H/2), H),\n",
    "                             testval=init_W_00)\n",
    "    \n",
    "    b_00 = pm.Normal('b_00', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_00)\n",
    "\n",
    "    W_10 = pm.Normal('W_10', 0, sigma=1,\n",
    "                            shape=(H, 1),\n",
    "                            testval=init_W_10)\n",
    "  \n",
    "    b_10 = pm.Normal('b_10', 0, sigma=1,\n",
    "                             shape=(1),\n",
    "                             testval=init_b_10)\n",
    "\n",
    "    # BNN 3\n",
    "    W_01 = pm.Normal('W_01', 0, sigma=1,\n",
    "                             shape=(int(H/2), H),\n",
    "                             testval=init_W_01)\n",
    "    \n",
    "    b_01 = pm.Normal('b_01', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_01)\n",
    "\n",
    "    W_11 = pm.Normal('W_11', 0, sigma=1,\n",
    "                            shape=(H, 1),\n",
    "                            testval=init_W_11)\n",
    "  \n",
    "    b_11 = pm.Normal('b_11', 0, sigma=1,\n",
    "                             shape=(1),\n",
    "                             testval=init_b_11)\n",
    "\n",
    "        \n",
    "    # Build neural-network using activation function\n",
    "    act_out_low = DNN(x_tensor_low, W_0, b_0, W_1, b_1, W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11 )[0]\n",
    "    act_out_high = DNN(x_tensor_high, W_0, b_0, W_1, b_1, W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11 )[1]\n",
    "\n",
    "    # LIKELIHOOD\n",
    "    out1 = pm.Normal('out1', mu=act_out_low, sigma=0.01, observed=y_tensor_low)\n",
    "    out2 = pm.Normal('out2', mu=act_out_high, sigma=0.01, observed=y_tensor_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building pymc model\n",
    "\n",
    "H = 10\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "init_W_0 = np.random.randn(X_l.shape[1], H).astype(floatX)\n",
    "init_W_1 = np.random.randn(H, H).astype(floatX)\n",
    "init_b_0 = np.random.randn(H).astype(floatX)\n",
    "init_b_1 = np.random.randn(H).astype(floatX)\n",
    "\n",
    "\n",
    "init_W_00 = np.random.randn(int(H/2), H).astype(floatX)\n",
    "init_W_10 = np.random.randn(H, 1).astype(floatX)\n",
    "init_b_00 = np.random.randn(H).astype(floatX)\n",
    "init_b_10 = np.random.randn(1).astype(floatX)\n",
    "\n",
    "init_W_01 = np.random.randn(int(H/2), H).astype(floatX)\n",
    "init_W_11 = np.random.randn(H, 1).astype(floatX)\n",
    "init_b_01 = np.random.randn(H).astype(floatX)\n",
    "init_b_11 = np.random.randn(1).astype(floatX)\n",
    "\n",
    "def DNN(x_tensor, W_0, b_0, W_1, b_1, W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11 ):\n",
    "    act_1 = pm.math.tanh(pm.math.dot(x_tensor,W_0)+b_0)\n",
    "    act_2 = pm.math.tanh(pm.math.dot(act_1,W_1)+b_1)\n",
    "    act_2_0 = act_2[:,:int(H/2)]\n",
    "    act_2_1 = act_2[:,int(-H/2):]\n",
    "    \n",
    "    act_10 = pm.math.tanh(pm.math.dot(act_2_0,W_00)+b_00)\n",
    "    act_20 = pm.math.dot(act_10,W_10)+b_10\n",
    "    \n",
    "    act_11 = pm.math.tanh(pm.math.dot(act_2_1,W_01)+b_01)\n",
    "    act_21 = pm.math.dot(act_11,W_11)+b_11\n",
    "    \n",
    "    return act_20,act_21\n",
    "\n",
    "with pm.Model() as neural_network:\n",
    "    # Trick: Turning inputs and outputs into shared variables \n",
    "    # It's still the same thing, but we can later change the values of the shared variable\n",
    "    # (to switch in the test-data later) and pymc3 will just use the new data.\n",
    "    # Kind-of like a pointer we can redirect.\n",
    "    # For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "    y_tensor_low = theano.shared(Y_l)\n",
    "    x_tensor_low = theano.shared(X_l)\n",
    "    \n",
    "    y_tensor_high = theano.shared(Y_h)\n",
    "    x_tensor_high = theano.shared(X_h)\n",
    "\n",
    "    #PRIOR\n",
    "    # BNN 1 \n",
    "    W_0 = pm.Normal('W_0', 0, sigma=1,\n",
    "                             shape=(X_l.shape[1], H),\n",
    "                             testval=init_W_0)\n",
    "    \n",
    "    b_0 = pm.Normal('b_0', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_0)\n",
    "\n",
    "    W_1 = pm.Normal('W_1', 0, sigma=1,\n",
    "                            shape=(H, H),\n",
    "                            testval=init_W_1)\n",
    "  \n",
    "    b_1 = pm.Normal('b_1', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_1)\n",
    "    \n",
    "    # BNN 2\n",
    "    W_00 = pm.Normal('W_00', 0, sigma=1,\n",
    "                             shape=(int(H/2), H),\n",
    "                             testval=init_W_00)\n",
    "    \n",
    "    b_00 = pm.Normal('b_00', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_00)\n",
    "\n",
    "    W_10 = pm.Normal('W_10', 0, sigma=1,\n",
    "                            shape=(H, 1),\n",
    "                            testval=init_W_10)\n",
    "  \n",
    "    b_10 = pm.Normal('b_10', 0, sigma=1,\n",
    "                             shape=(1),\n",
    "                             testval=init_b_10)\n",
    "\n",
    "    # BNN 3\n",
    "    W_01 = pm.Normal('W_01', 0, sigma=1,\n",
    "                             shape=(int(H/2), H),\n",
    "                             testval=init_W_01)\n",
    "    \n",
    "    b_01 = pm.Normal('b_01', 0, sigma=1,\n",
    "                             shape=(H),\n",
    "                             testval=init_b_01)\n",
    "\n",
    "    W_11 = pm.Normal('W_11', 0, sigma=1,\n",
    "                            shape=(H, 1),\n",
    "                            testval=init_W_11)\n",
    "  \n",
    "    b_11 = pm.Normal('b_11', 0, sigma=1,\n",
    "                             shape=(1),\n",
    "                             testval=init_b_11)\n",
    "\n",
    "        \n",
    "    # Build neural-network using activation function\n",
    "    act_out_low = DNN(x_tensor_low, W_0, b_0, W_1, b_1, W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11 )[0]\n",
    "    act_out_high = DNN(x_tensor_high, W_0, b_0, W_1, b_1, W_00, b_00, W_10, b_10, W_01, b_01, W_11, b_11 )[1]\n",
    "\n",
    "    # LIKELIHOOD\n",
    "    out1 = pm.Normal('out1', mu=act_out_low, sigma=0.01, observed=y_tensor_low)\n",
    "    out2 = pm.Normal('out2', mu=act_out_high, sigma=0.01, observed=y_tensor_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_0\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print ('W_'+str(0))\n",
    "print (type('W_'+str(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Normal in module pymc3.distributions.continuous:\n",
      "\n",
      "class Normal(pymc3.distributions.distribution.Continuous)\n",
      " |  Univariate normal log-likelihood.\n",
      " |  \n",
      " |  The pdf of this distribution is\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     f(x \\mid \\mu, \\tau) =\n",
      " |         \\sqrt{\\frac{\\tau}{2\\pi}}\n",
      " |         \\exp\\left\\{ -\\frac{\\tau}{2} (x-\\mu)^2 \\right\\}\n",
      " |  \n",
      " |  Normal distribution can be parameterized either in terms of precision\n",
      " |  or standard deviation. The link between the two parametrizations is\n",
      " |  given by\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |     \\tau = \\dfrac{1}{\\sigma^2}\n",
      " |  \n",
      " |  .. plot::\n",
      " |  \n",
      " |      import matplotlib.pyplot as plt\n",
      " |      import numpy as np\n",
      " |      import scipy.stats as st\n",
      " |      plt.style.use('seaborn-darkgrid')\n",
      " |      x = np.linspace(-5, 5, 1000)\n",
      " |      mus = [0., 0., 0., -2.]\n",
      " |      sigmas = [0.4, 1., 2., 0.4]\n",
      " |      for mu, sigma in zip(mus, sigmas):\n",
      " |          pdf = st.norm.pdf(x, mu, sigma)\n",
      " |          plt.plot(x, pdf, label=r'$\\mu$ = {}, $\\sigma$ = {}'.format(mu, sigma))\n",
      " |      plt.xlabel('x', fontsize=12)\n",
      " |      plt.ylabel('f(x)', fontsize=12)\n",
      " |      plt.legend(loc=1)\n",
      " |      plt.show()\n",
      " |  \n",
      " |  ========  ==========================================\n",
      " |  Support   :math:`x \\in \\mathbb{R}`\n",
      " |  Mean      :math:`\\mu`\n",
      " |  Variance  :math:`\\dfrac{1}{\\tau}` or :math:`\\sigma^2`\n",
      " |  ========  ==========================================\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  mu: float\n",
      " |      Mean.\n",
      " |  sigma: float\n",
      " |      Standard deviation (sigma > 0) (only required if tau is not specified).\n",
      " |  tau: float\n",
      " |      Precision (tau > 0) (only required if sigma is not specified).\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  .. code-block:: python\n",
      " |  \n",
      " |      with pm.Model():\n",
      " |          x = pm.Normal('x', mu=0, sigma=10)\n",
      " |  \n",
      " |      with pm.Model():\n",
      " |          x = pm.Normal('x', mu=0, tau=1/23)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Normal\n",
      " |      pymc3.distributions.distribution.Continuous\n",
      " |      pymc3.distributions.distribution.Distribution\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, mu=0, sigma=None, tau=None, sd=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  logcdf(self, value)\n",
      " |      Compute the log of the cumulative distribution function for Normal distribution\n",
      " |      at the specified value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value: numeric\n",
      " |          Value(s) for which log CDF is calculated. If the log CDF for multiple\n",
      " |          values are desired the values must be provided in a numpy array or theano tensor.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      TensorVariable\n",
      " |  \n",
      " |  logp(self, value)\n",
      " |      Calculate log-probability of Normal distribution at specified value.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value: numeric\n",
      " |          Value(s) for which log-probability is calculated. If the log probabilities for multiple\n",
      " |          values are desired the values must be provided in a numpy array or theano tensor\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      TensorVariable\n",
      " |  \n",
      " |  random(self, point=None, size=None)\n",
      " |      Draw random values from Normal distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      point: dict, optional\n",
      " |          Dict of variable values on which random values are to be\n",
      " |          conditioned (uses default point if not specified).\n",
      " |      size: int, optional\n",
      " |          Desired size of random sample (returns one sample if not\n",
      " |          specified).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __getnewargs__(self)\n",
      " |  \n",
      " |  __latex__ = _repr_latex_(self, name=None, dist=None)\n",
      " |      Magic method name for IPython to use for LaTeX formatting.\n",
      " |  \n",
      " |  default(self)\n",
      " |  \n",
      " |  get_test_val(self, val, defaults)\n",
      " |  \n",
      " |  getattr_value(self, val)\n",
      " |  \n",
      " |  logp_nojac(self, *args, **kwargs)\n",
      " |      Return the logp, but do not include a jacobian term for transforms.\n",
      " |      \n",
      " |      If we use different parametrizations for the same distribution, we\n",
      " |      need to add the determinant of the jacobian of the transformation\n",
      " |      to make sure the densities still describe the same distribution.\n",
      " |      However, MAP estimates are not invariant with respect to the\n",
      " |      parametrization, we need to exclude the jacobian terms in this case.\n",
      " |      \n",
      " |      This function should be overwritten in base classes for transformed\n",
      " |      distributions.\n",
      " |  \n",
      " |  logp_sum(self, *args, **kwargs)\n",
      " |      Return the sum of the logp values for the given observations.\n",
      " |      \n",
      " |      Subclasses can use this to improve the speed of logp evaluations\n",
      " |      if only the sum of the logp values is needed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  dist(*args, **kwargs) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __new__(cls, name, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pm.Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,3):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "print ( random.randint(0, 20)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm.math.tanh\n"
     ]
    }
   ],
   "source": [
    "act = 'tanh'\n",
    "print ('pm.math.'+str(act))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_low =[DNN(X_p, posterior_sampler()[0], posterior_sampler()[1] )[:,0][:,None].eval()\n",
    "#  for i in range(5)]\n",
    "# preds_low = np.asarray(preds_low)\n",
    "# outputs_low = preds_low.reshape(preds_low.shape[0],preds_low.shape[1])\n",
    "# print(np.shape(outputs_low))\n",
    "\n",
    "# preds_high =[DNN(X_p, posterior_sampler()[0], posterior_sampler()[1] )[:,1][:,None].eval()\n",
    "#  for i in range(5)]\n",
    "# preds_high = np.asarray(preds_high)\n",
    "# outputs_high = preds_high.reshape(preds_high.shape[0],preds_high.shape[1])\n",
    "# print(np.shape(outputs_high))\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 8))\n",
    "# ax = fig.add_subplot(111)\n",
    "\n",
    "# ax.plot(X_p, f_l(X_p), 'r--', label='True LF response')\n",
    "# ax.scatter(X_l, Y_l, color='red', marker='o', s=100, label='LF data')\n",
    "# ax.plot(X_p,outputs_low[0].T, lw=1, label='LF posterior draws', color='red')\n",
    "# ax.plot(X_p,outputs_low[1:].T, lw=1, color='red')\n",
    "\n",
    "# ax.plot(X_p, f_h(X_p), 'b--', label='True HF response')\n",
    "# ax.scatter(X_h, Y_h, color='blue', marker='x', s=100, label='HF data')\n",
    "# ax.plot(X_p,outputs_high[0].T, lw=1, label='HF posterior draws',color='blue')\n",
    "# ax.plot(X_p,outputs_high[1:].T, lw=1, color='blue')\n",
    "\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "y = 2\n",
    "for i, j in zip(range(x), range(y)):\n",
    "    print (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 1\n",
      "1 0\n",
      "1 1\n",
      "2 0\n",
      "2 1\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "y = 2\n",
    "import itertools\n",
    "for i, j in itertools.product(range(x), range(y)):\n",
    "    print (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsample = random.randint(0, nsamples) # random sample from trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot = 10 # number of prediction samples to plot\n",
    "\n",
    "def posterior_sampler():\n",
    "    Ws_post = []\n",
    "    bs_post = []\n",
    "    rsample = random.randint(0, nsamples) # random sample from trace\n",
    "    for i in range(len(d)-1):\n",
    "        W_post = trace['W_'+str(i)][ rsample ]\n",
    "        b_post = trace['b_'+str(i)][ rsample ]\n",
    "        Ws_post.append(W_post)\n",
    "        bs_post.append(b_post)\n",
    "    return Ws_post, bs_post\n",
    "\n",
    "preds_low = []\n",
    "preds_high = []\n",
    "for i in range(n_plot):\n",
    "    Ws_post, bs_post = posterior_sampler()\n",
    "    preds = DNN( X_p, Ws_post, bs_post ).eval() \n",
    "    preds_low.append( preds[:,0][:,None] )\n",
    "    preds_high.append( preds[:,1][:,None] )\n",
    "\n",
    "preds_low = np.asarray(preds_low)\n",
    "outputs_low = preds_low.reshape(preds_low.shape[0],preds_low.shape[1])\n",
    "print(np.shape(outputs_low))\n",
    "\n",
    "preds_high = np.asarray(preds_high)\n",
    "outputs_high = preds_high.reshape(preds_high.shape[0],preds_high.shape[1])\n",
    "print(np.shape(outputs_high))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(X_p, f_l(X_p), 'r--', label='True LF response')\n",
    "ax.scatter(X_l, Y_l, color='red', marker='o', s=100, label='LF data')\n",
    "ax.plot(X_p,outputs_low[0].T, lw=1, label='LF posterior draws', color='red')\n",
    "ax.plot(X_p,outputs_low[1:].T, lw=1, color='red')\n",
    "\n",
    "ax.plot(X_p, f_h(X_p), 'b--', label='True HF response')\n",
    "ax.scatter(X_h, Y_h, color='blue', marker='x', s=100, label='HF data')\n",
    "ax.plot(X_p,outputs_high[0].T, lw=1, label='HF posterior draws',color='blue')\n",
    "ax.plot(X_p,outputs_high[1:].T, lw=1, color='blue')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plot = 10 # number of prediction samples to plot\n",
    "nsamples = trace['W_0'].shape[0]\n",
    "\n",
    "def posterior_sampler():\n",
    "    Ws_post = []\n",
    "    bs_post = []\n",
    "    rsample = random.randint(0, nsamples) # random sample from trace\n",
    "    for i in range(len(d)-1):\n",
    "        W_post = trace['W_'+str(i)][ rsample ]\n",
    "        b_post = trace['b_'+str(i)][ rsample ]\n",
    "        Ws_post.append(W_post)\n",
    "        bs_post.append(b_post)\n",
    "    return Ws_post, bs_post\n",
    "\n",
    "preds_low = []\n",
    "preds_high = []\n",
    "for i in range(n_plot):\n",
    "    Ws_post, bs_post = posterior_sampler()\n",
    "    preds = DNN( X_p, Ws_post, bs_post ).eval() \n",
    "    preds_low.append( preds[:,0][:,None] )\n",
    "    preds_high.append( preds[:,1][:,None] )\n",
    "\n",
    "preds_low = np.asarray(preds_low)\n",
    "outputs_low = preds_low.reshape(preds_low.shape[0],preds_low.shape[1])\n",
    "print(np.shape(outputs_low))\n",
    "\n",
    "preds_high = np.asarray(preds_high)\n",
    "outputs_high = preds_high.reshape(preds_high.shape[0],preds_high.shape[1])\n",
    "print(np.shape(outputs_high))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(X_p, f_l(X_p), 'r--', label='True LF response')\n",
    "ax.scatter(X_l, Y_l, color='red', marker='o', s=100, label='LF data')\n",
    "ax.plot(X_p,outputs_low[0].T, lw=1, label='LF posterior draws', color='red')\n",
    "ax.plot(X_p,outputs_low[1:].T, lw=1, color='red')\n",
    "\n",
    "ax.plot(X_p, f_h(X_p), 'b--', label='True HF response')\n",
    "ax.scatter(X_h, Y_h, color='blue', marker='x', s=100, label='HF data')\n",
    "ax.plot(X_p,outputs_high[0].T, lw=1, label='HF posterior draws',color='blue')\n",
    "ax.plot(X_p,outputs_high[1:].T, lw=1, color='blue')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_10\n"
     ]
    }
   ],
   "source": [
    "print ('W_'+str(1)+'0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,1):\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img1 = plt.imread('<PATH_TO_IMG1>')\n",
    "img2 = plt.imread('<PATH_TO_IMG2>')\n",
    "\n",
    "NUM_ROWS = 1\n",
    "IMGs_IN_ROW = 2\n",
    "f, ax = plt.subplots(NUM_ROWS, IMGs_IN_ROW, figsize=(16,6))\n",
    "\n",
    "ax[0].imshow(img1)\n",
    "ax[1].imshow(img2)\n",
    "\n",
    "ax[0].set_title('image 1')\n",
    "ax[1].set_title('image 2')\n",
    "\n",
    "title = 'side by side view of images'\n",
    "f.suptitle(title, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize random weights and biases between each layer\n",
    "# BNN 1\n",
    "init_Ws = []\n",
    "init_bs = []\n",
    "for i in range(len(d)-1):\n",
    "    init_W = np.random.randn(d[i], d[i+1]).astype(floatX)\n",
    "    init_b = np.random.randn(d[i+1]).astype(floatX)\n",
    "    init_Ws.append(init_W)\n",
    "    init_bs.append(init_b)\n",
    "# BNN 2        \n",
    "init_Ws_0 = []\n",
    "init_bs_0 = []\n",
    "for i in range(len(d_0)-1):\n",
    "    init_W_0 = np.random.randn(d_0[i], d_0[i+1]).astype(floatX)\n",
    "    init_b_0 = np.random.randn(d_0[i+1]).astype(floatX)\n",
    "    init_Ws_0.append(init_W_0)\n",
    "    init_bs_0.append(init_b_0)\n",
    "# BNN 3\n",
    "init_Ws_1 = []\n",
    "init_bs_1 = []\n",
    "for i in range(len(d_1)-1):\n",
    "    init_W_1 = np.random.randn(d_1[i], d_1[i+1]).astype(floatX)\n",
    "    init_b_1 = np.random.randn(d_1[i+1]).astype(floatX)\n",
    "    init_Ws_1.append(init_W_1)\n",
    "    init_bs_1.append(init_b_1)\n",
    "\n",
    "    \n",
    "# DNN definition\n",
    "def DNN(x_tensor, Ws, bs, Ws_0, bs_0, Ws_1, bs_1):\n",
    "    \n",
    "    # BNN 1\n",
    "    h = act_func(pm.math.dot(x_tensor,Ws[0])+bs[0])\n",
    "    for i in range(1,len(d)-1):\n",
    "        h = act_func(pm.math.dot(h,Ws[i])+bs[i])\n",
    "    act_out_0 = h[:,:d_0[0]]\n",
    "    act_out_1 = h[:,-d_1[0]:]\n",
    "    \n",
    "    # BNN 2\n",
    "    h_0 = act_func(pm.math.dot(act_out_0,Ws_0[0])+bs_0[0])\n",
    "    for i in range(1,len(d_0)-2):\n",
    "        h_0 = act_func(pm.math.dot(h_0,Ws_0[i])+bs_0[i])\n",
    "    act_out_low = pm.math.dot(h_0,Ws_0[-1])+bs_0[-1]\n",
    "    \n",
    "    # BNN 3\n",
    "    h_1 = act_func(pm.math.dot(act_out_1,Ws_1[0])+bs_1[0])\n",
    "    for i in range(1,len(d_1)-2):\n",
    "        h_1 = act_func(pm.math.dot(h_1,Ws_1[i])+bs_1[i])\n",
    "    act_out_high = pm.math.dot(h_1,Ws_1[-1])+bs_1[-1]\n",
    "    \n",
    "    return act_out_low, act_out_high\n",
    "    \n",
    "        \n",
    "with pm.Model() as neural_network:\n",
    "    # Trick: Turning inputs and outputs into shared variables \n",
    "    # It's still the same thing, but we can later change the values of the shared variable\n",
    "    # (to switch in the test-data later) and pymc3 will just use the new data.\n",
    "    # Kind-of like a pointer we can redirect.\n",
    "    # For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html\n",
    "    y_tensor_low = theano.shared(Y_l)\n",
    "    x_tensor_low = theano.shared(X_l)\n",
    "    \n",
    "    y_tensor_high = theano.shared(Y_h)\n",
    "    x_tensor_high = theano.shared(X_h)\n",
    "\n",
    "    #PRIOR\n",
    "    # Weights and biases of BNN 1\n",
    "    Ws = []\n",
    "    bs = []\n",
    "    for i in range(len(d)-1):\n",
    "        W = pm.Normal('W_'+str(i), mu=0, sigma=1, shape=(d[i], d[i+1]), testval=init_Ws[i])\n",
    "        b = pm.Normal('b_'+str(i), mu=0, sigma=1, shape=(d[i+1]), testval=init_bs[i])\n",
    "        Ws.append(W)\n",
    "        bs.append(b)\n",
    "    # Weights and biases of BNN 2\n",
    "    Ws_0 = []\n",
    "    bs_0 = []\n",
    "    for i in range(len(d_0)-1):\n",
    "        W_0 = pm.Normal('W_'+str(i)+'0', mu=0, sigma=1, shape=(d_0[i], d_0[i+1]), testval=init_Ws_0[i])\n",
    "        b_0 = pm.Normal('b_'+str(i)+'0', mu=0, sigma=1, shape=(d_0[i+1]), testval=init_bs_0[i])\n",
    "        Ws_0.append(W_0)\n",
    "        bs_0.append(b_0)\n",
    "    # Weights and biases of BNN 3\n",
    "    Ws_1 = []\n",
    "    bs_1 = []\n",
    "    for i in range(len(d_1)-1):\n",
    "        W_1 = pm.Normal('W_'+str(i)+'1', mu=0, sigma=1, shape=(d_1[i], d_1[i+1]), testval=init_Ws_1[i])\n",
    "        b_1 = pm.Normal('b_'+str(i)+'1', mu=0, sigma=1, shape=(d_1[i+1]), testval=init_bs_1[i])\n",
    "        Ws_1.append(W_1)\n",
    "        bs_1.append(b_1)\n",
    "        \n",
    "    # Build neural-network using activation function\n",
    "    act_out_low = DNN(x_tensor_low, Ws, bs, Ws_0, bs_0, Ws_1, bs_1)[0]\n",
    "    act_out_high = DNN(x_tensor_high, Ws, bs, Ws_0, bs_0, Ws_1, bs_1)[1]\n",
    "\n",
    "    # LIKELIHOOD\n",
    "    out_low = pm.Normal('out_low', mu=act_out_low, sigma=0.01, observed=y_tensor_low)\n",
    "    out_high = pm.Normal('out_high', mu=act_out_high, sigma=0.01, observed=y_tensor_high)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
